{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xJM62lP7D_Ep"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alexwa/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# pip install wandb\n",
        "\n",
        "# import libraries\n",
        "import torch, torch.nn as nn\n",
        "import torchvision\n",
        "import wandb\n",
        "import torchvision as tv\n",
        "from torchvision import transforms\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaJWjOsiBeNA",
        "outputId": "4e3fa648-c222-43c4-8e25-2cf61055afde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/alexwa/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/alexwa/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdjdumpling\u001b[0m (\u001b[33mdjdumpling-yale\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HxDi2IdAFnav"
      },
      "outputs": [],
      "source": [
        "class FashionMNIST(nn.Module):\n",
        "\n",
        "  def __init__(self, batch_size = 64, resize = (28, 28), root = './data'):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.resize = resize\n",
        "    self.root = root\n",
        "\n",
        "    # data augmentation via color jitter and flip\n",
        "    color_aug = tv.transforms.ColorJitter(brightness = 0.25, contrast = 0.25, saturation = 0.25, hue = 0.25)\n",
        "    train_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            color_aug, \n",
        "            transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # no data augmentation for validation\n",
        "    val_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # access datasets within torchvision\n",
        "    self.train = tv.datasets.FashionMNIST(root=self.root, train=True , transform=train_transform, download=True)\n",
        "    self.val   = tv.datasets.FashionMNIST(root=self.root, train=False, transform=val_transform  , download=True)\n",
        "\n",
        "  def text_labels(self, indices):\n",
        "    labels = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "    return [labels[i] for i in indices]\n",
        "\n",
        "  def get_dataloader(self, train):\n",
        "    data = self.train if train else self.val\n",
        "\n",
        "    # data-iterator reads mini-batch of data\n",
        "    # key component for efficient performance\n",
        "    # exploit high-performance cmputing to avoid slowing down training loop\n",
        "    return torch.utils.data.DataLoader(data, self.batch_size, shuffle = train)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return self.get_dataloader(train = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CIFAR100(nn.Module):\n",
        "    def __init__(self, batch_size=64, resize=(32, 32), root=\"./data\"):\n",
        "        self.batch_size = batch_size\n",
        "        self.resize = resize\n",
        "        self.root = root\n",
        "\n",
        "        # CIFAR-100 normalization constants\n",
        "        mean = (0.5071, 0.4867, 0.4408)\n",
        "        std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "        color_aug = tv.transforms.ColorJitter(brightness = 0.25, contrast = 0.25, saturation = 0.25, hue = 0.25)\n",
        "\n",
        "        # train transforms: augment\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            color_aug, \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "        # val transforms: only resize + normalize\n",
        "        self.val_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "        self.train = torchvision.datasets.CIFAR100(root=self.root, train=True , transform=self.train_transform, download=True)\n",
        "        self.val   = torchvision.datasets.CIFAR100(root=self.root, train=False, transform=self.val_transform  , download=True)\n",
        "\n",
        "        self.classes = self.train.classes\n",
        "\n",
        "    def get_dataloader(self, train=True):\n",
        "        data = self.train if train else self.val\n",
        "        \n",
        "        return torch.utils.data.DataLoader(data, self.batch_size, shuffle = train)\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self.get_dataloader(train = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize weights properly\n",
        "def init_cnn(module):\n",
        "  if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
        "    nn.init.xavier_uniform_(module.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self, lr = 0.1, num_classes = 10, activation = 'LeakyReLU', pooling = 'AvgPool2d'):\n",
        "    super().__init__()\n",
        "    self.lr = lr\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    # hyperparameters for activation and pooling\n",
        "    if activation == 'LeakyReLU':\n",
        "      self.activation = nn.LeakyReLU(0.1)\n",
        "    elif activation == 'Tanh':\n",
        "      self.activation = nn.Tanh()\n",
        "    elif activation == 'GELU':\n",
        "      self.activation = nn.GELU()\n",
        "\n",
        "    if pooling == 'AvgPool2d':\n",
        "      self.pooling = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "    elif pooling == 'MaxPool2d':\n",
        "      self.pooling = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    if num_classes == 10:\n",
        "      self.net = nn.Sequential(\n",
        "        nn.LazyConv2d(6, kernel_size = 5, padding = 2),\n",
        "        self.activation,\n",
        "        self.pooling,\n",
        "        nn.LazyConv2d(16, kernel_size = 5),\n",
        "        self.activation,\n",
        "        self.pooling,\n",
        "        nn.Flatten(),\n",
        "        nn.LazyLinear(120),\n",
        "        self.activation,\n",
        "        nn.LazyLinear(84),\n",
        "        self.activation,\n",
        "        nn.LazyLinear(num_classes)\n",
        "      )\n",
        "    elif num_classes == 100:\n",
        "      self.net = nn.Sequential(\n",
        "        nn.LazyConv2d(6, kernel_size = 5, padding = 2),\n",
        "        self.activation,\n",
        "        self.pooling,\n",
        "        nn.LazyConv2d(16, kernel_size = 5),\n",
        "        self.activation,\n",
        "        self.pooling,\n",
        "        nn.Flatten(),\n",
        "        nn.LazyLinear(256),\n",
        "        self.activation,\n",
        "        nn.LazyLinear(128),\n",
        "        self.activation,\n",
        "        nn.LazyLinear(num_classes)\n",
        "      )\n",
        "      \n",
        "  # apply initialization to weights\n",
        "  def apply_init(self, inputs, init_fn):\n",
        "    self.net(torch.randn(*inputs, dtype=next(self.parameters()).dtype))\n",
        "    self.net.apply(init_fn)\n",
        "\n",
        "  # print shape of each layer\n",
        "  def layer_summary(self, X_shape):\n",
        "    X = torch.randn(*X_shape)\n",
        "    for layer in self.net:\n",
        "      X = layer(X)\n",
        "      print(f\"{layer.__class__.__name__} output shape: \\t{X.shape}\")\n",
        "\n",
        "  def train_model(self, data, max_epochs):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(self.parameters(), lr = self.lr)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "      self.train()\n",
        "      train_loss, train_acc, num_examples = 0.0, 0.0, 0\n",
        "\n",
        "      # back prop and optimizer step\n",
        "      for X, y in data.train_dataloader():\n",
        "        y_hat = self.net(X)\n",
        "        loss = loss_fn(y_hat, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * y.shape[0]\n",
        "        train_acc += (y_hat.argmax(dim = 1) == y).sum().item()\n",
        "        num_examples += y.shape[0]\n",
        "\n",
        "      train_loss /= num_examples\n",
        "      train_acc /= num_examples\n",
        "\n",
        "      val_loss, val_acc, precision, recall, f1 = self.evaluate_model(data.get_dataloader(train=False))\n",
        "\n",
        "      # log metrics only at the end of each epoch\n",
        "      if wandb.run is not None:\n",
        "        wandb.log({\n",
        "            \"epoch\":      epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\":  train_acc,\n",
        "            \"val_loss\":   val_loss,\n",
        "            \"val_acc\":    val_acc,\n",
        "            \"precision\":  precision,\n",
        "            \"recall\":     recall,\n",
        "            \"f1\":         f1,\n",
        "        }, step=epoch+1)\n",
        "\n",
        "  def evaluate_model(self, dataloader, epoch=None):\n",
        "    self.eval()\n",
        "    total_correct, total_samples = 0, 0\n",
        "    total_loss = 0.0\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # evaluate model with accuracy, loss, precision, recall, and f1 score\n",
        "    for X, y in dataloader:\n",
        "        outputs = self.net(X)\n",
        "        loss = loss_fn(outputs, y)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        total_correct += (preds == y).sum().item()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        total_samples += y.size(0)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    val_acc = total_correct / total_samples\n",
        "    val_loss = total_loss / total_samples\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return val_loss, val_acc, precision, recall, f1\n",
        "  \n",
        "  def total_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, activation=nn.ReLU(inplace=True)):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        out += self.shortcut(identity)\n",
        "        out = self.activation(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, lr=0.1, num_classes=10, activation='LeakyReLU', pooling='AvgPool2d'):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        if activation == 'LeakyReLU':\n",
        "            self.activation = nn.LeakyReLU(0.1)\n",
        "        elif activation == 'Tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'GELU':\n",
        "            self.activation = nn.GELU()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        if pooling == 'AvgPool2d':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        elif pooling == 'MaxPool2d':\n",
        "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            self.conv1,\n",
        "            self.bn1,\n",
        "            self.activation,\n",
        "            self.layer1,\n",
        "            self.layer2,\n",
        "            self.layer3,\n",
        "            self.layer4,\n",
        "            self.avgpool,\n",
        "            nn.Flatten(),\n",
        "            self.fc\n",
        "        )\n",
        "    \n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride, self.activation))\n",
        "        \n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels, 1, self.activation))\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def apply_init(self, inputs, init_fn):\n",
        "        self.net(torch.randn(*inputs, dtype=next(self.parameters()).dtype))\n",
        "        self.net.apply(init_fn)\n",
        "\n",
        "    def layer_summary(self, X_shape):\n",
        "        X = torch.randn(*X_shape)\n",
        "        for layer in self.net:\n",
        "            X = layer(X)\n",
        "            print(f\"{layer.__class__.__name__} output shape: \\t{X.shape}\")\n",
        "\n",
        "    def train_model(self, data, max_epochs):\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            self.train()\n",
        "            train_loss, train_acc, num_examples = 0.0, 0.0, 0\n",
        "\n",
        "            for X, y in data.train_dataloader():\n",
        "                y_hat = self.net(X)\n",
        "                loss = loss_fn(y_hat, y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * y.shape[0]\n",
        "                train_acc += (y_hat.argmax(dim=1) == y).sum().item()\n",
        "                num_examples += y.shape[0]\n",
        "\n",
        "            train_loss /= num_examples\n",
        "            train_acc /= num_examples\n",
        "\n",
        "            val_loss, val_acc, precision, recall, f1 = self.evaluate_model(data.get_dataloader(train=False))\n",
        "\n",
        "            if wandb.run is not None:\n",
        "                wandb.log({\n",
        "                    \"epoch\":      epoch + 1,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"train_acc\":  train_acc,\n",
        "                    \"val_loss\":   val_loss,\n",
        "                    \"val_acc\":    val_acc,\n",
        "                    \"precision\":  precision,\n",
        "                    \"recall\":     recall,\n",
        "                    \"f1\":         f1,\n",
        "                }, step=epoch+1)\n",
        "\n",
        "    def evaluate_model(self, dataloader, epoch=None):\n",
        "        self.eval()\n",
        "        total_correct, total_samples = 0, 0\n",
        "        total_loss = 0.0\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for X, y in dataloader:\n",
        "            outputs = self.net(X)\n",
        "            loss = loss_fn(outputs, y)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            total_correct += (preds == y).sum().item()\n",
        "            total_loss += loss.item() * y.size(0)\n",
        "            total_samples += y.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "        val_acc = total_correct / total_samples\n",
        "        val_loss = total_loss / total_samples\n",
        "\n",
        "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        return val_loss, val_acc, precision, recall, f1\n",
        "    \n",
        "    def total_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG98U2BFYLVV",
        "outputId": "ad208b2b-860e-4fe7-8c89-56446d30a675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d output shape: \ttorch.Size([1, 6, 28, 28])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 6, 28, 28])\n",
            "AvgPool2d output shape: \ttorch.Size([1, 6, 14, 14])\n",
            "Conv2d output shape: \ttorch.Size([1, 16, 10, 10])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 16, 10, 10])\n",
            "AvgPool2d output shape: \ttorch.Size([1, 16, 5, 5])\n",
            "Flatten output shape: \ttorch.Size([1, 400])\n",
            "Linear output shape: \ttorch.Size([1, 120])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 120])\n",
            "Linear output shape: \ttorch.Size([1, 84])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 84])\n",
            "Linear output shape: \ttorch.Size([1, 10])\n",
            "Total number of parameters: 61706\n"
          ]
        }
      ],
      "source": [
        "model = LeNet()\n",
        "# (batch_size, num_channels, height, width)\n",
        "model.layer_summary((1, 1, 28, 28))\n",
        "print(f'Total number of parameters: {model.total_parameters()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d output shape: \ttorch.Size([1, 64, 16, 16])\n",
            "BatchNorm2d output shape: \ttorch.Size([1, 64, 16, 16])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 64, 16, 16])\n",
            "Sequential output shape: \ttorch.Size([1, 64, 16, 16])\n",
            "Sequential output shape: \ttorch.Size([1, 128, 8, 8])\n",
            "Sequential output shape: \ttorch.Size([1, 256, 4, 4])\n",
            "Sequential output shape: \ttorch.Size([1, 512, 2, 2])\n",
            "AdaptiveAvgPool2d output shape: \ttorch.Size([1, 512, 1, 1])\n",
            "Flatten output shape: \ttorch.Size([1, 512])\n",
            "Linear output shape: \ttorch.Size([1, 10])\n",
            "Total number of parameters: 11172810\n"
          ]
        }
      ],
      "source": [
        "model = ResNet18()\n",
        "# (batch_size, num_channels, height, width)\n",
        "model.layer_summary((1, 1, 28, 28))\n",
        "print(f'Total number of parameters: {model.total_parameters()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LeNet on FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lu5DEMnPYZJS",
        "outputId": "89febb39-1900-4bea-da53-81e99869127f"
      },
      "outputs": [],
      "source": [
        "# figure out how to configure function to run with different parameters: model and dataset\n",
        "def run_experiment_leNet(activation, pooling, max_epochs = 25):\n",
        "    wandb.init(\n",
        "        project=\"CNNs-FashionMNIST-LeNet\",\n",
        "        name=f\"{activation}, {pooling}\",\n",
        "        config={\n",
        "            \"activation\": activation,\n",
        "            \"pooling\": pooling,\n",
        "            \"epochs\": max_epochs\n",
        "        }\n",
        "    )\n",
        "\n",
        "    config = wandb.config\n",
        "    data = FashionMNIST(batch_size=128)\n",
        "    model = LeNet(\n",
        "        activation=config.activation,\n",
        "        pooling=config.pooling,\n",
        "        lr=0.1\n",
        "    )\n",
        "    model.apply_init(next(iter(data.get_dataloader(True)))[0].shape, init_cnn)\n",
        "    model.train_model(data, max_epochs=config.epochs)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "activations = ['LeakyReLU', 'Tanh', 'GELU']\n",
        "pooling = ['AvgPool2d', 'MaxPool2d']\n",
        "\n",
        "for act in activations:\n",
        "    for pool in pooling:\n",
        "        run_experiment_leNet(activation = act, pooling = pool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet-18 on FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment_ResNet(activation, pooling, max_epochs = 25):\n",
        "    wandb.init(\n",
        "        project=\"CNNs-FashionMNIST-ResNet18\",\n",
        "        name=f\"{activation}, {pooling}\",\n",
        "        config={\n",
        "            \"activation\": activation,\n",
        "            \"pooling\": pooling,\n",
        "            \"epochs\": max_epochs\n",
        "        }\n",
        "    )\n",
        "\n",
        "    config = wandb.config\n",
        "    data = FashionMNIST(batch_size=128)\n",
        "    model = ResNet18(\n",
        "        activation=config.activation,\n",
        "        pooling=config.pooling,\n",
        "        lr=0.1\n",
        "    )\n",
        "    model.apply_init(next(iter(data.get_dataloader(True)))[0].shape, init_cnn)\n",
        "    model.train_model(data, max_epochs=config.epochs)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "activations = ['LeakyReLU', 'Tanh', 'GELU']\n",
        "pooling = ['AvgPool2d', 'MaxPool2d']\n",
        "\n",
        "for act in activations:\n",
        "    for pool in pooling:\n",
        "        run_experiment_ResNet(activation = act, pooling = pool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LeNet on CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d output shape: \ttorch.Size([1, 6, 32, 32])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 6, 32, 32])\n",
            "AvgPool2d output shape: \ttorch.Size([1, 6, 16, 16])\n",
            "Conv2d output shape: \ttorch.Size([1, 16, 12, 12])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 16, 12, 12])\n",
            "AvgPool2d output shape: \ttorch.Size([1, 16, 6, 6])\n",
            "Flatten output shape: \ttorch.Size([1, 576])\n",
            "Linear output shape: \ttorch.Size([1, 256])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 256])\n",
            "Linear output shape: \ttorch.Size([1, 128])\n",
            "LeakyReLU output shape: \ttorch.Size([1, 128])\n",
            "Linear output shape: \ttorch.Size([1, 100])\n",
            "Total number of parameters: 196380\n"
          ]
        }
      ],
      "source": [
        "model = LeNet(num_classes = 100)\n",
        "# (batch_size, num_channels, height, width)\n",
        "# CIFAR100 images are 32x32 with 3 color channels\n",
        "model.layer_summary((1, 3, 32, 32))\n",
        "print(f'Total number of parameters: {model.total_parameters()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdjdumpling\u001b[0m (\u001b[33mdjdumpling-yale\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/alexwa/Desktop/vision transformers/wandb/run-20250428_130443-uprdcjt4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/djdumpling-yale/CNNs-CIFAR100-LeNet/runs/uprdcjt4' target=\"_blank\">LeakyReLU, MaxPool2d</a></strong> to <a href='https://wandb.ai/djdumpling-yale/CNNs-CIFAR100-LeNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/djdumpling-yale/CNNs-CIFAR100-LeNet' target=\"_blank\">https://wandb.ai/djdumpling-yale/CNNs-CIFAR100-LeNet</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/djdumpling-yale/CNNs-CIFAR100-LeNet/runs/uprdcjt4' target=\"_blank\">https://wandb.ai/djdumpling-yale/CNNs-CIFAR100-LeNet/runs/uprdcjt4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alexwa/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/alexwa/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/alexwa/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# figure out how to configure function to run with different parameters: model and dataset\n",
        "def run_experiment_leNet(activation, pooling, max_epochs = 30):\n",
        "    wandb.init(\n",
        "        project=\"CNNs-CIFAR100-LeNet\",\n",
        "        name=f\"{activation}, {pooling}\",\n",
        "        config={\n",
        "            \"activation\": activation,\n",
        "            \"pooling\": pooling,\n",
        "            \"epochs\": max_epochs\n",
        "        }\n",
        "    )\n",
        "\n",
        "    config = wandb.config\n",
        "    data = CIFAR100(batch_size=128)\n",
        "    model = LeNet(\n",
        "        activation=config.activation,\n",
        "        pooling=config.pooling,\n",
        "        num_classes = 100,\n",
        "        lr=0.1\n",
        "    )\n",
        "    model.apply_init(next(iter(data.get_dataloader(True)))[0].shape, init_cnn)\n",
        "    model.train_model(data, max_epochs=config.epochs)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "activations = ['LeakyReLU', 'GELU']\n",
        "pooling = ['MaxPool2d']\n",
        "\n",
        "for act in activations:\n",
        "    for pool in pooling:\n",
        "        run_experiment_leNet(activation = act, pooling = pool)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
